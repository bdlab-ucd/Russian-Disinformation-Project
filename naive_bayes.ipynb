{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy items-Copy1.csv file into pandas dataframe and \n",
    "# replace all fields that are empty with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read dataset into pandas dataframe\n",
    "df_items = pd.read_csv('items-Copy1.csv')\n",
    "\n",
    "# replace field that's entirely space (or empty) with NaN\n",
    "df_items = df_items.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Black American racial experience is real. We s...\n",
       "1       The best costume for Halloween worth posting. ...\n",
       "2       Keep on whining and crying for your president,...\n",
       "3       End the whining and crying, end the riots with...\n",
       "4       Black girls are the definition of national gre...\n",
       "                              ...                        \n",
       "3007                      New ideas, old values. Like us!\n",
       "3008    Secured borders are a national priority. We ne...\n",
       "3009    Secured borders should be a top priority. We n...\n",
       "3010                                Bernie for president!\n",
       "3011    Secured borders are a national priority. Ameri...\n",
       "Name: description, Length: 3012, dtype: object"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go through all rows in dataframe and check for if \n",
    "1. anger and not fear is in tags\n",
    "2. fear and not anger is in tags\n",
    "3. anger and fear are both in tags\n",
    "4. neither anger or fear are in tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label word</th>\n",
       "      <th>label numerical</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Black American racial experience is real. We s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Black girls are the definition of national gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Imma stay here comfy and untouched, yet workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>There is a disgusting video Circulating on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Unapologetically melaneted Kings and Queens ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>New ideas, old values. Like us!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Secured borders are a national priority. We ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Secured borders should be a top priority. We n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Bernie for president!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>neither</td>\n",
       "      <td>3</td>\n",
       "      <td>Secured borders are a national priority. Ameri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2417 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label word  label numerical  \\\n",
       "0          fear                1   \n",
       "4       neither                3   \n",
       "5       neither                3   \n",
       "6       neither                3   \n",
       "10      neither                3   \n",
       "...         ...              ...   \n",
       "3007    neither                3   \n",
       "3008    neither                3   \n",
       "3009    neither                3   \n",
       "3010    neither                3   \n",
       "3011    neither                3   \n",
       "\n",
       "                                            description  \n",
       "0     Black American racial experience is real. We s...  \n",
       "4     Black girls are the definition of national gre...  \n",
       "5     Imma stay here comfy and untouched, yet workin...  \n",
       "6     There is a disgusting video Circulating on the...  \n",
       "10    Unapologetically melaneted Kings and Queens ar...  \n",
       "...                                                 ...  \n",
       "3007                    New ideas, old values. Like us!  \n",
       "3008  Secured borders are a national priority. We ne...  \n",
       "3009  Secured borders should be a top priority. We n...  \n",
       "3010                              Bernie for president!  \n",
       "3011  Secured borders are a national priority. Ameri...  \n",
       "\n",
       "[2417 rows x 3 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list_word = [] # holds list of labels in word form\n",
    "labels_list_numerical = [] # holds list of labels in numerical form\n",
    "descriptions_list = [] # holds list of descriptions\n",
    "\n",
    "for i, row in df_items.iterrows():\n",
    "    row['tag'] = row['tag'].lower()\n",
    "    if 'anger' in row['tag'] and not 'fear' in row['tag']:\n",
    "        labels_list_word.append('anger') # anger\n",
    "        descriptions_list.append(row['description'])\n",
    "        labels_list_numerical.append(0) # 0\n",
    "    elif not 'anger' in row['tag'] and 'fear' in row['tag']:\n",
    "        labels_list_word.append('fear') # fear\n",
    "        descriptions_list.append(row['description'])\n",
    "        labels_list_numerical.append(1) # 1\n",
    "    elif 'anger' in row['tag'] and 'fear' in row['tag']:\n",
    "        labels_list_word.append('both') # both\n",
    "        descriptions_list.append(row['description'])\n",
    "        labels_list_numerical.append(2) # 2\n",
    "    else:\n",
    "        labels_list_word.append('neither') # neither\n",
    "        descriptions_list.append(row['description'])\n",
    "        labels_list_numerical.append(3) # 3\n",
    "\n",
    "df = pd.DataFrame() # create empty dataframe\n",
    "df['label word'] = labels_list_word # append labels_list_word to df with column header 'label word'\n",
    "df['label numerical'] = labels_list_numerical # append labels_list_numerical to df with column header 'label numerical'\n",
    "df['description'] = descriptions_list # append descriptions_list to df with column header 'description'\n",
    "df.loc[df['label numerical'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go through all rows in dataframe and check for if\n",
    "1. anger is in tags\n",
    "2. anger is not in tags\n",
    "\n",
    "## This is a binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label word</th>\n",
       "      <th>label numerical</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>The best costume for Halloween worth posting. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Keep on whining and crying for your president,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>End the whining and crying, end the riots with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Art imitates life art. This photo is great.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>In America, racial oppression and racism were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Officials of the Highlands High School are inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember folks, dance and music is a large par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>Indigenous people unite!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label word  label numerical  \\\n",
       "1         anger                1   \n",
       "2         anger                1   \n",
       "3         anger                1   \n",
       "7         anger                1   \n",
       "8         anger                1   \n",
       "...         ...              ...   \n",
       "2964      anger                1   \n",
       "2976      anger                1   \n",
       "2989      anger                1   \n",
       "2995      anger                1   \n",
       "3002      anger                1   \n",
       "\n",
       "                                            description  \n",
       "1     The best costume for Halloween worth posting. ...  \n",
       "2     Keep on whining and crying for your president,...  \n",
       "3     End the whining and crying, end the riots with...  \n",
       "7           Art imitates life art. This photo is great.  \n",
       "8     In America, racial oppression and racism were ...  \n",
       "...                                                 ...  \n",
       "2964  Officials of the Highlands High School are inv...  \n",
       "2976  Protect the 2nd. Without it, you won't have an...  \n",
       "2989  Protect the 2nd. Without it, you won't have an...  \n",
       "2995  Remember folks, dance and music is a large par...  \n",
       "3002                           Indigenous people unite!  \n",
       "\n",
       "[781 rows x 3 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list_word = [] # holds list of labels in word form\n",
    "labels_list_numerical = [] # holds list of labels in numerical form\n",
    "descriptions_list = [] # holds list of descriptions\n",
    "\n",
    "# binary classification so either anger or not\n",
    "for i, row in df_items.iterrows():\n",
    "    row['tag'] = row['tag'].lower() # convert tags to lowercase\n",
    "    if 'anger' in row['tag']:\n",
    "        labels_list_word.append('anger') # anger\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(1) # 1\n",
    "    else:\n",
    "        labels_list_word.append('none') # not anger\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(0) # 0\n",
    "\n",
    "df_anger = pd.DataFrame() # create empty dataframe\n",
    "df_anger['label word'] = labels_list_word # append labels_list_word to df with column header 'label word'\n",
    "df_anger['label numerical'] = labels_list_numerical # append labels_list_numerical to df with column header 'label numerical'\n",
    "df_anger['description'] = descriptions_list # append descriptions_list to df with column header 'description'\n",
    "df_anger.loc[df_anger['label word'] == 'anger']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go through all rows in dataframe and check for if\n",
    "1. fear is in tags\n",
    "2. fear is not in tags\n",
    "\n",
    "## This is a binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label word</th>\n",
       "      <th>label numerical</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Black American racial experience is real. We s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Watch this heart-piercing story about a racial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>People are genuinely scared for their futures!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>For years, white supremacists in the Dothan, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>The cop beat this man like he was a runaway sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Give your online shopping a fresh start with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>People really need to understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>Today Americans are able to elect a president ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label word  label numerical  \\\n",
       "0          fear                1   \n",
       "11         fear                1   \n",
       "15         fear                1   \n",
       "23         fear                1   \n",
       "25         fear                1   \n",
       "...         ...              ...   \n",
       "2973       fear                1   \n",
       "2976       fear                1   \n",
       "2987       fear                1   \n",
       "2989       fear                1   \n",
       "3005       fear                1   \n",
       "\n",
       "                                            description  \n",
       "0     Black American racial experience is real. We s...  \n",
       "11    Watch this heart-piercing story about a racial...  \n",
       "15    People are genuinely scared for their futures!...  \n",
       "23    For years, white supremacists in the Dothan, A...  \n",
       "25    The cop beat this man like he was a runaway sl...  \n",
       "...                                                 ...  \n",
       "2973  Give your online shopping a fresh start with t...  \n",
       "2976  Protect the 2nd. Without it, you won't have an...  \n",
       "2987                   People really need to understand  \n",
       "2989  Protect the 2nd. Without it, you won't have an...  \n",
       "3005  Today Americans are able to elect a president ...  \n",
       "\n",
       "[317 rows x 3 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list_word = [] # holds list of labels in word form\n",
    "labels_list_numerical = [] # holds list of labels in numerical form\n",
    "descriptions_list = [] # holds list of descriptions\n",
    "\n",
    "# binary classification so either fear or not\n",
    "for i, row in df_items.iterrows():\n",
    "    row['tag'] = row['tag'].lower() # convert tags to lowercase\n",
    "    if 'fear' in row['tag']:\n",
    "        labels_list_word.append('fear') # fear\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(1) # 1\n",
    "    else:\n",
    "        labels_list_word.append('none') # not anger\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(0) # 0\n",
    "\n",
    "df_fear = pd.DataFrame() # create empty dataframe\n",
    "df_fear['label word'] = labels_list_word # append labels_list_word to df with column header 'label word'\n",
    "df_fear['label numerical'] = labels_list_numerical # append labels_list_numerical to df with column header 'label numerical'\n",
    "df_fear['description'] = descriptions_list # append descriptions_list to df with column header 'description'\n",
    "df_fear.loc[df_fear['label word'] == 'fear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go through all rows in dataframe and check for if\n",
    "1. anger and fear are both in tags\n",
    "2. neither are in the tags\n",
    "\n",
    "## This is a binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label word</th>\n",
       "      <th>label numerical</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>People are genuinely scared for their futures!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>For years, white supremacists in the Dothan, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>The cop beat this man like he was a runaway sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>There is a disgusting video Circulating on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Black American racial experience is real. We s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Black Matters. Black community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Fast-growing black community. Latest news and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Officials of the Highlands High School are inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>both</td>\n",
       "      <td>1</td>\n",
       "      <td>Protect the 2nd. Without it, you won't have an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label word  label numerical  \\\n",
       "15         both                1   \n",
       "23         both                1   \n",
       "25         both                1   \n",
       "40         both                1   \n",
       "41         both                1   \n",
       "...         ...              ...   \n",
       "2930       both                1   \n",
       "2954       both                1   \n",
       "2964       both                1   \n",
       "2976       both                1   \n",
       "2989       both                1   \n",
       "\n",
       "                                            description  \n",
       "15    People are genuinely scared for their futures!...  \n",
       "23    For years, white supremacists in the Dothan, A...  \n",
       "25    The cop beat this man like he was a runaway sl...  \n",
       "40    There is a disgusting video Circulating on the...  \n",
       "41    Black American racial experience is real. We s...  \n",
       "...                                                 ...  \n",
       "2930                    Black Matters. Black community.  \n",
       "2954  Fast-growing black community. Latest news and ...  \n",
       "2964  Officials of the Highlands High School are inv...  \n",
       "2976  Protect the 2nd. Without it, you won't have an...  \n",
       "2989  Protect the 2nd. Without it, you won't have an...  \n",
       "\n",
       "[186 rows x 3 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list_word = [] # holds list of labels in word form\n",
    "labels_list_numerical = [] # holds list of labels in numerical form\n",
    "descriptions_list = [] # holds list of descriptions\n",
    "\n",
    "# binary classification so either fear or not\n",
    "for i, row in df_items.iterrows():\n",
    "    row['tag'] = row['tag'].lower() # convert tags to lowercase\n",
    "    if 'fear' in row['tag'] and 'anger' in row['tag']:\n",
    "        labels_list_word.append('both') # both\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(1) # 1\n",
    "    else:\n",
    "        labels_list_word.append('none') # neither\n",
    "        descriptions_list.append(row['description']) # add description\n",
    "        labels_list_numerical.append(0) # 0\n",
    "\n",
    "df_both = pd.DataFrame() # create empty dataframe\n",
    "df_both['label word'] = labels_list_word # append labels_list_word to df with column header 'label word'\n",
    "df_both['label numerical'] = labels_list_numerical # append labels_list_numerical to df with column header 'label numerical'\n",
    "df_both['description'] = descriptions_list # append descriptions_list to df with column header 'description'\n",
    "df_both.loc[df_both['label word'] == 'both']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train test split for classification for anger or not anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set for anger: 3012\n",
      "Number of rows in the training set for anger: 2259\n",
      "Number of rows in the test set for anger: 753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_anger, X_test_anger, Y_train_anger, Y_test_anger = train_test_split(df_anger['description'], \n",
    "                                                                            df_anger['label numerical'], \n",
    "                                                                            random_state=1)\n",
    "\n",
    "print('Number of rows in the total set for anger: {}'.format(df_anger.shape[0]))\n",
    "print('Number of rows in the training set for anger: {}'.format(X_train_anger.shape[0]))\n",
    "print('Number of rows in the test set for anger: {}'.format(X_test_anger.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance of CountVectorizer\n",
    "# Fit training data and return matrix\n",
    "# transform testing data and return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate Countvectorizer method\n",
    "count_vector_anger = CountVectorizer()\n",
    "\n",
    "# fit training data and return matrix\n",
    "training_data_anger = count_vector_anger.fit_transform(X_train_anger)\n",
    "\n",
    "# transform testing data and return matrix\n",
    "testing_data_anger = count_vector_anger.transform(X_test_anger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize MultinomialNB from sklearn to create a naive bayes classifier and form predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_anger = MultinomialNB()\n",
    "naive_bayes_anger.fit(training_data_anger, Y_train_anger)\n",
    "\n",
    "predictions_anger = naive_bayes_anger.predict(testing_data_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.1.0 in /usr/local/lib/python3.7/site-packages (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib==3.1.0) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.17.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1.0) (41.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib==3.1.0) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib==3.1.0 # use this version of matplotlib as other version causes problems with seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out classification report for anger vs not anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "print('Classification report for anger classification: ')\n",
    "print('---------------------------------------------------------- ')\n",
    "print(classification_report(Y_test_anger, predictions_anger, target_names = ['anger', 'none']))\n",
    "print('---------------------------------------------------------- ')\n",
    "print('Accuracy score: ', format(accuracy_score(predictions_anger, Y_test_anger)))\n",
    "print('Precision score: ', format(precision_score(predictions_anger, Y_test_anger)))\n",
    "print('Recall score: ', format(recall_score(predictions_anger, Y_test_anger)))\n",
    "print('F1 score: ', format(f1_score(predictions_anger, Y_test_anger)))\n",
    "print('---------------------------------------------------------- ')\n",
    "\n",
    "labels = ['anger', 'none']\n",
    "cm = confusion_matrix(list(Y_test_anger), predictions_anger)\n",
    "print(\"Confusion matrix anger: \\n\")\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix Anger')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_anger.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train test split for classification for fear or not fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_fear, X_test_fear, Y_train_fear, Y_test_fear = train_test_split(df_fear['description'], \n",
    "                                                                            df_fear['label numerical'], \n",
    "                                                                            random_state=1)\n",
    "\n",
    "print('Number of rows in the total set for fear: {}'.format(df_fear.shape[0]))\n",
    "print('Number of rows in the training set for fear: {}'.format(X_train_fear.shape[0]))\n",
    "print('Number of rows in the test set for fear: {}'.format(X_test_fear.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance of CountVectorizer\n",
    "# Fit training data and return matrix\n",
    "# transform testing data and return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate Countvectorizer method\n",
    "count_vector_fear = CountVectorizer()\n",
    "\n",
    "# fit training data and return matrix\n",
    "training_data_fear = count_vector_fear.fit_transform(X_train_fear)\n",
    "\n",
    "# transform testing data and return matrix\n",
    "testing_data_fear = count_vector_fear.transform(X_test_fear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize MultinomialNB from sklearn to create a naive bayes classifier and form predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_fear = MultinomialNB()\n",
    "naive_bayes_fear.fit(training_data_fear, Y_train_fear)\n",
    "\n",
    "predictions_fear = naive_bayes_fear.predict(testing_data_fear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out classification report for fear vs not fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "print('Classification report for fear classification: ')\n",
    "print('---------------------------------------------------------- ')\n",
    "print(classification_report(Y_test_fear, predictions_fear, target_names = ['fear', 'none']))\n",
    "print('---------------------------------------------------------- ')\n",
    "print('Accuracy score: ', format(accuracy_score(predictions_fear, Y_test_fear)))\n",
    "print('Precision score: ', format(precision_score(predictions_fear, Y_test_fear)))\n",
    "print('Recall score: ', format(recall_score(predictions_fear, Y_test_fear)))\n",
    "print('F1 score: ', format(f1_score(predictions_fear, Y_test_fear)))\n",
    "print('---------------------------------------------------------- ')\n",
    "\n",
    "labels = ['fear', 'none']\n",
    "cm = confusion_matrix(list(Y_test_fear), predictions_fear)\n",
    "print(\"Confusion matrix fear: \\n\")\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix Fear')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_fear.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train test split for classification for both or not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_both, X_test_both, Y_train_both, Y_test_both = train_test_split(df_both['description'], \n",
    "                                                                            df_both['label numerical'], \n",
    "                                                                            random_state=1)\n",
    "\n",
    "print('Number of rows in the total set for both: {}'.format(df_both.shape[0]))\n",
    "print('Number of rows in the training set for both: {}'.format(X_train_both.shape[0]))\n",
    "print('Number of rows in the test set for both: {}'.format(X_test_both.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance of CountVectorizer\n",
    "# Fit training data and return matrix\n",
    "# transform testing data and return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate Countvectorizer method\n",
    "count_vector_both = CountVectorizer()\n",
    "\n",
    "# fit training data and return matrix\n",
    "training_data_both = count_vector_both.fit_transform(X_train_both)\n",
    "\n",
    "# transform testing data and return matrix\n",
    "testing_data_both = count_vector_both.transform(X_test_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize MultinomialNB from sklearn to create a naive bayes classifier and form predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_both = MultinomialNB()\n",
    "naive_bayes_both.fit(training_data_both, Y_train_both)\n",
    "\n",
    "predictions_both = naive_bayes_both.predict(testing_data_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print out classification report for both vs not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "print('Classification report for both classification: ')\n",
    "print('---------------------------------------------------------- ')\n",
    "print(classification_report(Y_test_both, predictions_both, target_names = ['both', 'none']))\n",
    "print('---------------------------------------------------------- ')\n",
    "print('Accuracy score: ', format(accuracy_score(predictions_both, Y_test_both)))\n",
    "print('Precision score: ', format(precision_score(predictions_both, Y_test_both)))\n",
    "print('Recall score: ', format(recall_score(predictions_both, Y_test_both)))\n",
    "print('F1 score: ', format(f1_score(predictions_both, Y_test_both)))\n",
    "print('---------------------------------------------------------- ')\n",
    "\n",
    "labels = ['both', 'none']\n",
    "cm = confusion_matrix(list(Y_test_both), predictions_both)\n",
    "print(\"Confusion matrix both: \\n\")\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix Both')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_both.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity measures the proportion of actual positives that are correctly identified as such. In probability notation: P(T+|D+) = TP / (TP+FN).\n",
    "\n",
    "# Specificity measures the proportion of actual negatives that are correctly identified as such. In probability notation: P(T-|D-) = TN / (TN + FP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    '''\n",
    "    Description:\n",
    "        Takes in ground truth and predicted values and through a series\n",
    "        of comparisons determines the number of True Positives (TP), False\n",
    "        Positives (FP), True Negatives (TN), False Negatives (FN) and \n",
    "        returns these values in a tuple.\n",
    "    Input:\n",
    "        y_actual: Actual values of y set\n",
    "        y_hat: Predicted values of y set\n",
    "    Output:\n",
    "        (TP, FP, TN, FN): Tuple of performance measures\n",
    "    '''\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    # Go through all values\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1: # True Positive\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]: # False Positive\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0: # True Negative\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]: # False Negative\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity and Specificity measures for anger, fear and both classifications before any adjustments for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_anger, FP_anger, TN_anger, FN_anger = perf_measure(list(Y_test_anger), list(predictions_anger))\n",
    "sensitivity_anger = TP_anger / (TP_anger+FN_anger)\n",
    "specificity_anger = TN_anger / (TN_anger + FP_anger)\n",
    "print(\"Sensitivity Measure for Anger Classification: {sensitivity_anger}\".format(sensitivity_anger=str(sensitivity_anger)))\n",
    "print(\"Specificity Measure for Anger Classification: {specificity_anger}\".format(specificity_anger=str(specificity_anger)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_fear, FP_fear, TN_fear, FN_fear = perf_measure(list(Y_test_fear), list(predictions_fear))\n",
    "sensitivity_fear = TP_fear / (TP_fear+FN_fear)\n",
    "specificity_fear = TN_fear / (TN_fear + FP_fear)\n",
    "print(\"Sensitivity Measure for Fear Classification: {sensitivity_fear}\".format(sensitivity_fear=str(sensitivity_fear)))\n",
    "print(\"Specificity Measure for Fear Classification: {specificity_fear}\".format(specificity_fear=str(specificity_fear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_both, FP_both, TN_both, FN_both = perf_measure(list(Y_test_both), list(predictions_both))\n",
    "sensitivity_both = TP_both / (TP_both+FN_both)\n",
    "specificity_both = TN_both / (TN_both + FP_both)\n",
    "print(\"Sensitivity Measure for Both Classification: {sensitivity_both}\".format(sensitivity_both=str(sensitivity_both)))\n",
    "print(\"Specificity Measure for Both Classification: {specificity_both}\".format(specificity_both=str(specificity_both)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try under-sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# remove (2231 - 781) = 1450 negative samples from overall set for anger \n",
    "# this will ensure that the number of positive and negative samples are equal\n",
    "pos_anger_df = df_anger.loc[df_anger['label numerical'] == 1]\n",
    "\n",
    "neg_anger_df = df_anger.loc[df_anger['label numerical'] == 0].sample(n=781, random_state=42)\n",
    "\n",
    "normalized_anger_df = pd.concat([pos_anger_df, neg_anger_df])\n",
    "\n",
    "#plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('label numerical', data=normalized_anger_df)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat steps from above for anger classification again after undersamplilng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_anger, X_test_anger, Y_train_anger, Y_test_anger = train_test_split(normalized_anger_df['description'], \n",
    "                                                                            normalized_anger_df['label numerical'], \n",
    "                                                                            random_state=1)\n",
    "\n",
    "print('Number of rows in the total set for anger: {}'.format(normalized_anger_df.shape[0]))\n",
    "print('Number of rows in the training set for anger: {}'.format(X_train_anger.shape[0]))\n",
    "print('Number of rows in the test set for anger: {}'.format(X_test_anger.shape[0]))\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# instantiate Countvectorizer method\n",
    "count_vector_anger = CountVectorizer()\n",
    "\n",
    "# fit training data and return matrix\n",
    "training_data_anger = count_vector_anger.fit_transform(X_train_anger)\n",
    "\n",
    "# transform testing data and return matrix\n",
    "testing_data_anger = count_vector_anger.transform(X_test_anger)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "naive_bayes_anger = MultinomialNB()\n",
    "naive_bayes_anger.fit(training_data_anger, Y_train_anger)\n",
    "\n",
    "predictions_anger = naive_bayes_anger.predict(testing_data_anger)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "print('Classification report for undersampled anger classification: ')\n",
    "print('---------------------------------------------------------- ')\n",
    "print(classification_report(Y_test_anger, predictions_anger, target_names = ['anger', 'none']))\n",
    "print('---------------------------------------------------------- ')\n",
    "print('Accuracy score: ', format(accuracy_score(predictions_anger, Y_test_anger)))\n",
    "print('Precision score: ', format(precision_score(predictions_anger, Y_test_anger)))\n",
    "print('Recall score: ', format(recall_score(predictions_anger, Y_test_anger)))\n",
    "print('F1 score: ', format(f1_score(predictions_anger, Y_test_anger)))\n",
    "print('---------------------------------------------------------- ')\n",
    "\n",
    "labels = ['anger', 'none']\n",
    "cm = confusion_matrix(list(Y_test_anger), predictions_anger)\n",
    "print(\"Confusion matrix undersampled anger: \\n\")\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix Undersampled Anger')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_undersampled_anger.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cv(splits, X, Y, pipeline, average_method):\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        lr_fit = pipeline.fit(X[train], Y[train])\n",
    "        prediction = lr_fit.predict(X[test])\n",
    "        scores = lr_fit.score(X[test],Y[test])\n",
    "        \n",
    "        accuracy.append(scores * 100)\n",
    "        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('              negative     positive')\n",
    "        print('precision:',precision_score(Y[test], prediction, average=None))\n",
    "        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('recall:   ',recall_score(Y[test], prediction, average=None))\n",
    "        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n",
    "        print('-'*50)\n",
    "\n",
    "    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
    "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
    "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import unidecode\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def spacy_cleaner(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[spacy_cleaner(t) for t in df_anger.description[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anger['clean_text'] = [spacy_cleaner(t) for t in df_anger['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SMOTE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# ROS_pipeline = make_pipeline(tvec, RandomOverSampler(random_state=777),lr)\n",
    "SMOTE_pipeline = make_pipeline(tvec, SMOTE(random_state=777), lr)\n",
    "\n",
    "tv = TfidfVectorizer(stop_words=None, max_features=100000)\n",
    "testing_tfidf = tv.fit_transform(df_anger['clean_text'])\n",
    "\n",
    "'''ros = RandomOverSampler(random_state=777)\n",
    "X_ROS, y_ROS = ros.fit_sample(testing_tfidf, df_anger['label numerical'])\n",
    "pd.DataFrame(testing_tfidf.todense(), columns=tv.get_feature_names())\n",
    "\n",
    "pd.DataFrame(X_ROS.todense(), columns=tv.get_feature_names())\n",
    "\n",
    "y_ROS'''\n",
    "\n",
    "df_labelnumerical_temp = df_anger['label numerical']\n",
    "\n",
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(testing_tfidf, df_anger['label numerical'])\n",
    "pd.DataFrame(X_SMOTE.todense(), columns=tv.get_feature_names())\n",
    "\n",
    "#lr_cv(5, df_anger.clean_text, df_labelnumerical_temp, SMOTE_pipeline, 'macro')\n",
    "\n",
    "##################################################################\n",
    "\n",
    "X_train_anger, X_test_anger, Y_train_anger, Y_test_anger = train_test_split(X_SMOTE, \n",
    "                                                                            y_SMOTE, \n",
    "                                                                            random_state=1)\n",
    "\n",
    "#print('Number of rows in the total set for anger: {}'.format(normalized_anger_df.shape[0]))\n",
    "print('Number of rows in the training set for anger: {}'.format(X_train_anger.shape[0]))\n",
    "print('Number of rows in the test set for anger: {}'.format(X_test_anger.shape[0]))\n",
    "\n",
    "###################################################################################\n",
    "'''\n",
    "# instantiate Countvectorizer method\n",
    "count_vector_anger = CountVectorizer()\n",
    "\n",
    "# fit training data and return matrix\n",
    "training_data_anger = count_vector_anger.fit_transform(X_train_anger)\n",
    "\n",
    "# transform testing data and return matrix\n",
    "testing_data_anger = count_vector_anger.transform(X_test_anger)\n",
    "'''\n",
    "###################################################################################\n",
    "\n",
    "naive_bayes_anger = MultinomialNB()\n",
    "naive_bayes_anger.fit(X_train_anger, Y_train_anger)\n",
    "\n",
    "predictions_anger = naive_bayes_anger.predict(X_test_anger)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "print('Classification report for oversampled anger classification: ')\n",
    "print('---------------------------------------------------------- ')\n",
    "print(classification_report(Y_test_anger, predictions_anger, target_names = ['anger', 'none']))\n",
    "print('---------------------------------------------------------- ')\n",
    "print('Accuracy score: ', format(accuracy_score(predictions_anger, Y_test_anger)))\n",
    "print('Precision score: ', format(precision_score(predictions_anger, Y_test_anger)))\n",
    "print('Recall score: ', format(recall_score(predictions_anger, Y_test_anger)))\n",
    "print('F1 score: ', format(f1_score(predictions_anger, Y_test_anger)))\n",
    "print('---------------------------------------------------------- ')\n",
    "\n",
    "labels = ['anger', 'none']\n",
    "cm = confusion_matrix(list(Y_test_anger), predictions_anger)\n",
    "print(\"Confusion matrix oversampled anger: \\n\")\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix Uversampled Anger')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix_oversampled_anger.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy items-Copy1.csv file into pandas dataframe and \n",
    "# replace all fields that are empty with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            description  index\n",
      "0     Black American racial experience is real. We s...      0\n",
      "1     The best costume for Halloween worth posting. ...      1\n",
      "2     Keep on whining and crying for your president,...      2\n",
      "3     End the whining and crying, end the riots with...      3\n",
      "4     Black girls are the definition of national gre...      4\n",
      "...                                                 ...    ...\n",
      "3007                    New ideas, old values. Like us!   3007\n",
      "3008  Secured borders are a national priority. We ne...   3008\n",
      "3009  Secured borders should be a top priority. We n...   3009\n",
      "3010                              Bernie for president!   3010\n",
      "3011  Secured borders are a national priority. Ameri...   3011\n",
      "\n",
      "[3012 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read dataset into pandas dataframe\n",
    "df_items = pd.read_csv('items-Copy1.csv')\n",
    "\n",
    "# replace field that's entirely space (or empty) with NaN\n",
    "df_items = df_items.replace(np.nan, '', regex=True)\n",
    "\n",
    "data_text = df_items[['description']]\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black American racial experience is real. We s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best costume for Halloween worth posting. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keep on whining and crying for your president,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>End the whining and crying, end the riots with...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black girls are the definition of national gre...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  index\n",
       "0  Black American racial experience is real. We s...      0\n",
       "1  The best costume for Halloween worth posting. ...      1\n",
       "2  Keep on whining and crying for your president,...      2\n",
       "3  End the whining and crying, end the riots with...      3\n",
       "4  Black girls are the definition of national gre...      4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to perform the following steps:\n",
    "1. Tokenization -- split text into sequences and sentences into words.\n",
    "    Lowercase all words and remove punctuation.\n",
    "2. Remove words with fewer than 3 characters\n",
    "3. Remove stopwords\n",
    "4. Lemmatize words -- words in third person are changed to first person\n",
    "    and verbs in past and future tenses are changed into present.\n",
    "5. Stem words --  words are reduced to root form.\n",
    "\n",
    "## gensim and nltk libraries will be used to \n",
    "## implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tobbylie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function in order to lemmatize and stem\n",
    "# preprocess on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display before and after of preprocessing on\n",
    "# example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['End', 'the', 'whining', 'and', 'crying,', 'end', 'the', 'riots', 'with', 'these', 'same', 'white\\npeople(democrats),', 'who', 'will', 'not', 'march', 'with', 'you', 'when', 'your', 'son,', 'daughter', 'or\\nloved', 'one', 'gets', 'shot', 'for', 'being', 'Black.\\n\\nI', 'opted', 'out', 'of', 'the', 'elections', 'and', 'i', 'had', 'many', 'Black', 'people', 'all', 'up', 'in', 'my', 'case,\\ntelling', 'me', 'how', 'i', 'was', 'wrong', 'and', 'how', 'i', 'needed', 'to', 'make', 'my', 'vote', 'count', 'for', 'Mrs.\\nClinton,', 'all', 'just', 'to', 'make', 'sure', 'Trump', \"doesn't\", 'become', 'president.\\n\\nWell', 'now', 'all', 'i', 'want', 'to', 'say', 'is', 'get', 'over', 'Trump', 'and', 'Clinton,', 'they', 'are', 'rich', 'white\\npeople,', 'the', 'elite', 'who', 'were', 'born', 'into', 'and', 'are', 'a', 'part', 'of', 'the', 'system.\\n\\nI', 'dream', 'of', 'liberation,', 'not', 'staying', 'a', 'slave', 'under', 'Hillary', 'or', 'Trump,', 'so', 'forget\\nyour', 'Childish', 'rioting,', 'no', 'one', 'has', 'shot!\\n\\n#wakeup', '#organize', '#joshuabeal', '#altonsterling', '#TerenceCrutCher\\n#westandtogether', '#blacklivesmatter', '#unite4justice', '#novem', 'ber8\\n#elections2016', '#black', '#melanin', '#blackisbeautiful', '#blacklivesmatter\\n#blackgirlmagic', '#USA', '#blackandproud', '#democrat', '#republioan\\n#Africanamerican', '#unite4justice', '#vote', '#breakingnews']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['whine', 'cri', 'riot', 'white', 'peopl', 'democrat', 'march', 'daughter', 'love', 'get', 'shoot', 'black', 'opt', 'elect', 'black', 'peopl', 'case', 'tell', 'wrong', 'need', 'vote', 'count', 'clinton', 'sure', 'trump', 'presid', 'want', 'trump', 'clinton', 'rich', 'white', 'peopl', 'elit', 'bear', 'dream', 'liber', 'stay', 'slave', 'hillari', 'trump', 'forget', 'childish', 'riot', 'shoot', 'wakeup', 'organ', 'joshuab', 'altonsterl', 'terencecrutch', 'westandtogeth', 'unit', 'justic', 'novem', 'elect', 'black', 'melanin', 'blackgirlmag', 'blackandproud', 'democrat', 'republioan', 'africanamerican', 'unit', 'justic', 'vote', 'breakingnew']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 3].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess all description field text\n",
    "# Save result in 'processed_docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [black, american, racial, experi, real, suppor...\n",
       "1    [best, costum, halloween, worth, post, power, ...\n",
       "2    [whine, cri, presid, wasn, elect, imma, care, ...\n",
       "3    [whine, cri, riot, white, peopl, democrat, mar...\n",
       "4                [black, girl, definit, nation, great]\n",
       "5    [imma, stay, comfi, untouch, work, hard, care,...\n",
       "6    [disgust, video, circul, internet, show, unide...\n",
       "7                           [imit, life, photo, great]\n",
       "8    [america, racial, oppress, racism, ancient, hi...\n",
       "9    [donald, john, trump, elect, presid, unit, sta...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['description'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bag of Words with Data set\n",
    "## Start with creating dictionary with number of times\n",
    "## a word appears in training set then filter out tokens\n",
    "## that appear in:\n",
    "1. less than 15 documents OR\n",
    "2. more than 0.5 documents\n",
    "### * Keep only first 100000 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize doc2bow from gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1),\n",
       " (16, 1),\n",
       " (113, 1),\n",
       " (135, 1),\n",
       " (147, 1),\n",
       " (161, 2),\n",
       " (233, 1),\n",
       " (252, 1),\n",
       " (360, 1),\n",
       " (375, 1),\n",
       " (406, 3),\n",
       " (438, 1),\n",
       " (478, 1),\n",
       " (494, 1),\n",
       " (509, 1),\n",
       " (518, 1),\n",
       " (596, 1),\n",
       " (602, 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview BoW from sample preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 (\"peopl\") appears 1 time.\n",
      "Word 16 (\"educ\") appears 1 time.\n",
      "Word 113 (\"month\") appears 1 time.\n",
      "Word 135 (\"free\") appears 1 time.\n",
      "Word 147 (\"prove\") appears 1 time.\n",
      "Word 161 (\"communiti\") appears 2 time.\n",
      "Word 233 (\"believ\") appears 1 time.\n",
      "Word 252 (\"proud\") appears 1 time.\n",
      "Word 360 (\"meet\") appears 1 time.\n",
      "Word 375 (\"charlott\") appears 1 time.\n",
      "Word 406 (\"legal\") appears 3 time.\n",
      "Word 438 (\"have\") appears 1 time.\n",
      "Word 478 (\"announc\") appears 1 time.\n",
      "Word 494 (\"basi\") appears 1 time.\n",
      "Word 509 (\"provid\") appears 1 time.\n",
      "Word 518 (\"night\") appears 1 time.\n",
      "Word 596 (\"empow\") appears 1 time.\n",
      "Word 602 (\"focus\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_3000 = bow_corpus[3000]\n",
    "for i in range(len(bow_doc_3000)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_3000[i][0], \n",
    "                                               dictionary[bow_doc_3000[i][0]], \n",
    "bow_doc_3000[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tf-idf model using models.tfidfmodel on\n",
    "# bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "# from pprint import pprint\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LDA model using gensim.models.LdaMulticore\n",
    "# Save to 'lda_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each topic, explore the words occuring in that\n",
    "# topic and the relative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.029*\"black\" + 0.019*\"like\" + 0.016*\"life\" + 0.015*\"join\" + 0.014*\"need\" + 0.014*\"year\" + 0.013*\"free\" + 0.013*\"stop\" + 0.011*\"peopl\" + 0.011*\"communiti\"\n",
      "Topic: 1 \n",
      "Words: 0.086*\"black\" + 0.044*\"communiti\" + 0.037*\"matter\" + 0.025*\"support\" + 0.024*\"join\" + 0.019*\"life\" + 0.018*\"live\" + 0.017*\"patriot\" + 0.015*\"polic\" + 0.015*\"year\"\n",
      "Topic: 2 \n",
      "Words: 0.027*\"defens\" + 0.025*\"school\" + 0.025*\"free\" + 0.023*\"seif\" + 0.022*\"join\" + 0.021*\"class\" + 0.019*\"friend\" + 0.018*\"bring\" + 0.018*\"feel\" + 0.017*\"event\"\n",
      "Topic: 3 \n",
      "Words: 0.027*\"black\" + 0.024*\"https\" + 0.020*\"facebook\" + 0.020*\"follow\" + 0.017*\"america\" + 0.015*\"william\" + 0.015*\"peopl\" + 0.012*\"twitter\" + 0.012*\"communiti\" + 0.012*\"proud\"\n",
      "Topic: 4 \n",
      "Words: 0.021*\"repost\" + 0.018*\"like\" + 0.016*\"offic\" + 0.013*\"woman\" + 0.013*\"live\" + 0.012*\"news\" + 0.012*\"transgend\" + 0.011*\"black\" + 0.011*\"polic\" + 0.010*\"shoot\"\n",
      "Topic: 5 \n",
      "Words: 0.024*\"https\" + 0.023*\"follow\" + 0.016*\"american\" + 0.015*\"peopl\" + 0.013*\"instagram\" + 0.013*\"twitter\" + 0.012*\"facebook\" + 0.011*\"unit\" + 0.010*\"know\" + 0.010*\"muslim\"\n",
      "Topic: 6 \n",
      "Words: 0.066*\"black\" + 0.025*\"polic\" + 0.024*\"peopl\" + 0.014*\"white\" + 0.013*\"shoot\" + 0.012*\"offic\" + 0.011*\"video\" + 0.011*\"kill\" + 0.011*\"communiti\" + 0.009*\"american\"\n",
      "Topic: 7 \n",
      "Words: 0.045*\"free\" + 0.040*\"stop\" + 0.028*\"black\" + 0.027*\"facemus\" + 0.025*\"music\" + 0.025*\"info\" + 0.025*\"onlin\" + 0.025*\"musicfb\" + 0.021*\"player\" + 0.019*\"browser\"\n",
      "Topic: 8 \n",
      "Words: 0.027*\"right\" + 0.022*\"american\" + 0.018*\"polic\" + 0.016*\"peopl\" + 0.014*\"share\" + 0.013*\"black\" + 0.013*\"join\" + 0.012*\"home\" + 0.012*\"clinton\" + 0.010*\"hillari\"\n",
      "Topic: 9 \n",
      "Words: 0.033*\"polic\" + 0.022*\"join\" + 0.022*\"stop\" + 0.020*\"brutal\" + 0.018*\"nation\" + 0.017*\"support\" + 0.017*\"black\" + 0.014*\"immigr\" + 0.014*\"refuge\" + 0.014*\"offic\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt distinguishing topics using words in each\n",
    "# topic and their corresponding weights\n",
    "\n",
    "## Run LDA using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.021*\"black\" + 0.017*\"matter\" + 0.013*\"polic\" + 0.013*\"girl\" + 0.012*\"year\" + 0.012*\"live\" + 0.010*\"nation\" + 0.009*\"join\" + 0.008*\"educ\" + 0.008*\"say\"\n",
      "Topic: 1 Word: 0.018*\"join\" + 0.018*\"beauti\" + 0.018*\"black\" + 0.015*\"proud\" + 0.014*\"want\" + 0.013*\"heart\" + 0.013*\"america\" + 0.011*\"texa\" + 0.011*\"live\" + 0.011*\"girl\"\n",
      "Topic: 2 Word: 0.016*\"immigr\" + 0.014*\"welcom\" + 0.014*\"music\" + 0.014*\"free\" + 0.014*\"facemus\" + 0.013*\"listen\" + 0.012*\"communiti\" + 0.012*\"meme\" + 0.011*\"need\" + 0.011*\"support\"\n",
      "Topic: 3 Word: 0.023*\"muslim\" + 0.023*\"polic\" + 0.017*\"brutal\" + 0.013*\"right\" + 0.013*\"black\" + 0.013*\"agre\" + 0.011*\"free\" + 0.011*\"proud\" + 0.010*\"https\" + 0.010*\"train\"\n",
      "Topic: 4 Word: 0.014*\"stand\" + 0.013*\"racism\" + 0.012*\"tell\" + 0.011*\"black\" + 0.011*\"student\" + 0.011*\"truth\" + 0.010*\"injustic\" + 0.010*\"goal\" + 0.009*\"love\" + 0.009*\"south\"\n",
      "Topic: 5 Word: 0.027*\"like\" + 0.015*\"real\" + 0.015*\"black\" + 0.014*\"https\" + 0.014*\"news\" + 0.013*\"american\" + 0.013*\"refuge\" + 0.012*\"stop\" + 0.011*\"facebook\" + 0.010*\"african\"\n",
      "Topic: 6 Word: 0.013*\"black\" + 0.013*\"unit\" + 0.012*\"peopl\" + 0.012*\"stay\" + 0.011*\"time\" + 0.010*\"white\" + 0.010*\"kill\" + 0.010*\"stop\" + 0.009*\"video\" + 0.009*\"chang\"\n",
      "Topic: 7 Word: 0.027*\"communiti\" + 0.020*\"black\" + 0.020*\"amend\" + 0.020*\"patriot\" + 0.020*\"support\" + 0.019*\"defend\" + 0.019*\"lover\" + 0.019*\"gun\" + 0.018*\"join\" + 0.017*\"matter\"\n",
      "Topic: 8 Word: 0.046*\"repost\" + 0.034*\"veteran\" + 0.017*\"support\" + 0.016*\"mexican\" + 0.015*\"like\" + 0.014*\"women\" + 0.011*\"peopl\" + 0.011*\"communiti\" + 0.011*\"black\" + 0.010*\"nativ\"\n",
      "Topic: 9 Word: 0.019*\"free\" + 0.019*\"stop\" + 0.018*\"care\" + 0.017*\"black\" + 0.016*\"join\" + 0.013*\"polic\" + 0.012*\"browser\" + 0.012*\"onlin\" + 0.012*\"facemus\" + 0.012*\"player\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance by classifying sample\n",
    "# document using LDA BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proud',\n",
       " 'announc',\n",
       " 'initi',\n",
       " 'focus',\n",
       " 'provid',\n",
       " 'free',\n",
       " 'legal',\n",
       " 'educ',\n",
       " 'empow',\n",
       " 'peopl',\n",
       " 'strengthen',\n",
       " 'communiti',\n",
       " 'believ',\n",
       " 'have',\n",
       " 'legal',\n",
       " 'workshop',\n",
       " 'month',\n",
       " 'basi',\n",
       " 'prove',\n",
       " 'benefici',\n",
       " 'tangibl',\n",
       " 'communiti',\n",
       " 'meet',\n",
       " 'charlott',\n",
       " 'legal',\n",
       " 'night']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9590823650360107\t \n",
      "Topic: 0.027*\"black\" + 0.024*\"https\" + 0.020*\"facebook\" + 0.020*\"follow\" + 0.017*\"america\" + 0.015*\"william\" + 0.015*\"peopl\" + 0.012*\"twitter\" + 0.012*\"communiti\" + 0.012*\"proud\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[3000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance by classifying sample\n",
    "# document using LDA tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9590772986412048\t \n",
      "Topic: 0.016*\"immigr\" + 0.014*\"welcom\" + 0.014*\"music\" + 0.014*\"free\" + 0.014*\"facemus\" + 0.013*\"listen\" + 0.012*\"communiti\" + 0.012*\"meme\" + 0.011*\"need\" + 0.011*\"support\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[3000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5861672759056091\t Topic: 0.027*\"black\" + 0.024*\"https\" + 0.020*\"facebook\" + 0.020*\"follow\" + 0.017*\"america\"\n",
      "Score: 0.280438631772995\t Topic: 0.027*\"right\" + 0.022*\"american\" + 0.018*\"polic\" + 0.016*\"peopl\" + 0.014*\"share\"\n",
      "Score: 0.016677675768733025\t Topic: 0.086*\"black\" + 0.044*\"communiti\" + 0.037*\"matter\" + 0.025*\"support\" + 0.024*\"join\"\n",
      "Score: 0.016677502542734146\t Topic: 0.066*\"black\" + 0.025*\"polic\" + 0.024*\"peopl\" + 0.014*\"white\" + 0.013*\"shoot\"\n",
      "Score: 0.016675498336553574\t Topic: 0.024*\"https\" + 0.023*\"follow\" + 0.016*\"american\" + 0.015*\"peopl\" + 0.013*\"instagram\"\n",
      "Score: 0.016674697399139404\t Topic: 0.027*\"defens\" + 0.025*\"school\" + 0.025*\"free\" + 0.023*\"seif\" + 0.022*\"join\"\n",
      "Score: 0.016674106940627098\t Topic: 0.045*\"free\" + 0.040*\"stop\" + 0.028*\"black\" + 0.027*\"facemus\" + 0.025*\"music\"\n",
      "Score: 0.01667366363108158\t Topic: 0.029*\"black\" + 0.019*\"like\" + 0.016*\"life\" + 0.015*\"join\" + 0.014*\"need\"\n",
      "Score: 0.016671176999807358\t Topic: 0.033*\"polic\" + 0.022*\"join\" + 0.022*\"stop\" + 0.020*\"brutal\" + 0.018*\"nation\"\n",
      "Score: 0.01666977070271969\t Topic: 0.021*\"repost\" + 0.018*\"like\" + 0.016*\"offic\" + 0.013*\"woman\" + 0.013*\"live\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Black Facebook employees complain racism, discrimination have gotten worse'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

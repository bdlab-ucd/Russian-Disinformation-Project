{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Here is the sentence I want embeddings for.\n",
      "Tokenized:  ['here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.']\n",
      "Token IDs:  [2182, 2003, 1996, 6251, 1045, 2215, 7861, 8270, 4667, 2015, 2005, 1012]\n"
     ]
    }
   ],
   "source": [
    "text = 'Here is the sentence I want embeddings for.'\n",
    "print('Original: ', text)\n",
    "print('Tokenized: ', tokenizer.tokenize(text))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Here is the sentence I want embeddings for.\n",
      "Token IDs:  tensor([[ 101, 2182, 2003, 1996, 6251, 1045, 2215, 7861, 8270, 4667, 2015, 2005,\n",
      "         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=64,\n",
    "                            pad_to_max_length=True,\n",
    "                            return_attention_maxk=True,\n",
    "                            return_tensors='pt',\n",
    "                            )\n",
    "\n",
    "print('Original: ', text)\n",
    "print('Token IDs: ', encoded_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', 'the', 'bank', 'robber', 'went', 'home', 'after', 'hanging', 'out', 'for', 'a', 'bit', 'then', 'the', 'em', '##bed', '##ding', '##s', 'happened', '.', '[SEP]']\n",
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "went          2,253\n",
      "home          2,188\n",
      "after         2,044\n",
      "hanging       5,689\n",
      "out           2,041\n",
      "for           2,005\n",
      "a             1,037\n",
      "bit           2,978\n",
      "then          2,059\n",
      "the           1,996\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "##s           2,015\n",
      "happened      3,047\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. The bank robber went home after hanging out for a bit then the embeddings happened.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "print(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print(segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers:  13  (initial embeddings + 12 BERT layers)\n",
      "Number of batches:  1\n",
      "Number of tokens:  33\n",
      "Number of hidden units:  768\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers: \", len(hidden_states), \" (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print(\"Number of batches: \", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print(\"Number of tokens: \", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print(\"Number of hidden units: \", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = hidden_states[layer_i][batch_i][token_i]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 33, 768])\n"
     ]
    }
   ],
   "source": [
    "print('       Type of hidden_states: ', type(hidden_states))\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 33, 768])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 33, 768])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 13, 768])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 33 x 3072\n"
     ]
    }
   ],
   "source": [
    "token_vecs_cat = []\n",
    "\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    token_vecs_cat.append(cat_vec)\n",
    "    \n",
    "print('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 the\n",
      "22 bank\n",
      "23 robber\n",
      "24 went\n",
      "25 home\n",
      "26 after\n",
      "27 hanging\n",
      "28 out\n",
      "29 for\n",
      "30 a\n",
      "31 bit\n",
      "32 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0756, -0.1104, -0.2482,  ..., -0.1289,  0.1924, -0.0267]),\n",
       " tensor([-0.2198, -0.1743, -0.2079,  ..., -0.7645,  0.0613, -0.5130]),\n",
       " tensor([-0.2962, -0.1225,  0.1134,  ..., -0.4554,  0.3013, -0.4266]),\n",
       " tensor([ 0.4303, -0.5760, -0.1539,  ..., -0.0116,  0.6551,  0.1940]),\n",
       " tensor([-0.3584, -0.3591,  0.3202,  ...,  0.2359,  0.2882, -1.0571]),\n",
       " tensor([-0.4706, -0.8805, -0.2219,  ...,  0.4580,  0.8436, -0.5998]),\n",
       " tensor([ 0.8812, -0.3844, -0.0615,  ...,  0.5664,  0.4199,  0.0870]),\n",
       " tensor([ 0.6422, -0.6802,  0.0587,  ..., -0.0094,  0.6543,  1.4138]),\n",
       " tensor([ 0.3488,  0.1282,  0.1586,  ..., -0.6438,  0.1559,  0.3896]),\n",
       " tensor([-0.7185, -0.7969, -0.2912,  ...,  0.0277,  0.4213, -0.5132]),\n",
       " tensor([ 8.1067e-01, -3.0953e-01,  4.4433e-03,  ...,  2.1753e-01,\n",
       "          2.3836e-01, -7.2937e-05]),\n",
       " tensor([ 0.2369, -0.3897,  0.1312,  ..., -0.9324,  0.6611, -1.1054]),\n",
       " tensor([-0.4636, -0.3601,  0.2483,  ..., -0.4101,  0.0529, -0.7894]),\n",
       " tensor([-0.4173, -0.2478,  0.2103,  ...,  0.1606, -0.1942, -0.5230]),\n",
       " tensor([ 0.2396, -0.1939,  0.0758,  ..., -0.0339, -1.0835, -0.4497]),\n",
       " tensor([-0.2915,  0.0078,  0.1426,  ..., -0.2249, -0.3150, -0.5017]),\n",
       " tensor([-0.0848, -0.0168, -0.1281,  ...,  0.5402,  0.3495, -0.5519]),\n",
       " tensor([ 0.7600,  0.3375, -0.3510,  ...,  0.1602, -0.4158, -0.9203]),\n",
       " tensor([ 0.6439,  0.5743, -0.2497,  ..., -0.1432, -0.4596, -0.4522]),\n",
       " tensor([ 0.3250, -0.1273,  0.0106,  ..., -0.5737, -0.3872, -0.0748]),\n",
       " tensor([ 0.4544,  0.0069, -0.0685,  ..., -0.2657, -0.1632,  0.0520]),\n",
       " tensor([-0.5631, -0.3311, -0.1459,  ..., -0.2311,  0.2679, -0.7844]),\n",
       " tensor([ 1.1929e+00, -3.5856e-01,  2.8069e-02,  ..., -3.0397e-01,\n",
       "          2.5455e-05, -2.4569e-02]),\n",
       " tensor([ 0.4761, -0.4406,  0.0729,  ..., -1.1139,  0.5056, -1.3914]),\n",
       " tensor([-0.0822, -0.6707,  0.6274,  ..., -0.4863, -0.2326, -0.1112]),\n",
       " tensor([ 0.8592, -0.0941,  0.6198,  ...,  0.1536, -0.6753, -0.0770]),\n",
       " tensor([ 0.0134, -0.4079, -0.0176,  ..., -0.8383, -0.8587,  0.4296]),\n",
       " tensor([ 0.9861, -1.0721,  0.1512,  ...,  0.6737, -0.9825, -0.4531]),\n",
       " tensor([ 0.4528, -0.5242,  0.2791,  ...,  0.3120, -0.5523, -0.8006]),\n",
       " tensor([ 0.1367, -0.5690,  0.6251,  ..., -0.4511, -0.9917,  0.1787]),\n",
       " tensor([-0.3517, -0.7723,  0.0180,  ...,  0.1937, -0.9399,  0.6490]),\n",
       " tensor([ 0.7231, -1.0626,  0.0735,  ..., -0.9090, -0.1629, -0.8527]),\n",
       " tensor([ 0.2706, -0.0762, -0.2000,  ..., -0.0597, -0.0175, -0.0635])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape:  torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-3.0080e-02, -4.0224e-01, -7.2126e-02,  3.3965e-01,  7.1512e-01,\n",
       "        -7.5021e-02,  2.2201e-01,  8.1727e-01,  1.1844e-01, -1.5126e-01,\n",
       "         3.6631e-01, -2.7246e-01, -3.8922e-01,  4.8129e-01, -6.4335e-01,\n",
       "         1.0510e-01,  4.4636e-02,  1.3050e-01,  4.4962e-01,  2.5615e-01,\n",
       "        -1.7316e-01, -1.3809e-01, -1.5470e-02, -1.8410e-02,  1.8184e-01,\n",
       "         4.2995e-01,  1.3439e-01,  1.6523e-01, -4.2481e-01,  1.8253e-01,\n",
       "         4.0812e-01,  1.9076e-01,  6.1621e-01, -4.4735e-01,  7.0374e-02,\n",
       "        -4.7848e-01, -1.0551e-01, -7.2108e-02, -2.3482e-01,  6.6996e-01,\n",
       "        -2.6026e-01, -1.1057e-01, -2.7519e-02, -6.1230e-02,  2.3163e-01,\n",
       "        -2.4750e-01,  2.0334e-01, -4.7155e-01, -1.6020e-01, -4.6486e-01,\n",
       "        -3.2217e-01,  3.3791e-01, -5.1706e-01, -2.7718e-01,  1.7601e-01,\n",
       "         6.2790e-01, -3.6744e-01, -7.8739e-01,  1.3813e-01, -2.1470e-02,\n",
       "         1.1718e-01, -1.5134e-01,  1.5122e-01, -4.0524e-01, -3.4906e-01,\n",
       "        -3.9230e-01,  3.5299e-01, -4.7119e-02, -1.1161e-01,  1.6177e-01,\n",
       "         5.7321e-02,  3.8511e-01, -2.9723e-01, -2.3265e-01,  1.7723e-02,\n",
       "         5.4168e-01,  3.7634e-01,  1.8108e-01, -5.6712e-01,  6.2474e-02,\n",
       "         1.3928e-01,  2.8144e-01, -9.9204e-02, -4.5174e-03,  1.7868e-01,\n",
       "         1.8361e-01,  9.1423e-02,  2.3132e-01,  1.4094e-01,  1.9879e-01,\n",
       "        -6.0431e-01,  2.4404e-01, -1.5077e-01,  8.5461e-02,  1.3446e-03,\n",
       "        -6.5600e-02, -2.1979e-01,  2.0739e-01, -1.9863e-01,  1.1322e-01,\n",
       "         4.2625e-01, -8.6955e-01, -2.4067e-01,  3.1227e-02,  1.6134e-01,\n",
       "        -8.4030e-02, -1.1109e-01,  3.0833e-01, -4.4876e-01, -5.1192e-01,\n",
       "        -6.9834e-02,  1.3826e-01, -3.9443e-02, -4.5844e-01,  7.9645e-02,\n",
       "         5.1962e-01,  2.2104e-01, -4.6073e-01, -2.6183e-01,  1.3475e-01,\n",
       "        -4.8290e-01,  2.9108e-01,  8.0357e-03,  1.4097e-01, -5.3072e-02,\n",
       "        -1.2998e-01, -1.6975e-01, -3.3414e-01,  2.7785e-01, -7.3576e-02,\n",
       "         6.0634e-01,  3.5500e-01,  3.1279e-01, -1.3067e-01,  1.6904e-01,\n",
       "        -1.2531e-02, -9.8694e-02, -3.1806e-01, -8.6054e-01,  3.0825e-03,\n",
       "        -3.1605e-01, -1.3709e-01,  1.1234e-01,  6.4251e-01, -2.4973e-01,\n",
       "         3.1893e-02, -2.1731e-02, -4.9831e-01, -9.7696e-02,  6.8943e-01,\n",
       "        -3.8709e-01,  5.9496e-02, -3.3455e-01, -1.3570e-01,  4.9054e-01,\n",
       "         1.1582e-01, -5.5013e-01, -7.0425e-03, -1.1978e-01, -4.8305e-02,\n",
       "         3.8388e-01,  5.6145e-01, -3.1498e-01,  8.7113e-02, -6.5095e-03,\n",
       "        -1.9137e-01, -6.8839e-02,  2.3366e-01, -2.5927e-01,  6.9613e-02,\n",
       "        -2.5195e-01,  2.0516e-01,  5.3539e-01,  5.2722e-02,  4.9359e-01,\n",
       "         3.1743e-02, -2.5734e-01, -2.2137e-01,  1.6283e-01,  4.9950e-02,\n",
       "        -4.5575e-01,  1.5192e-01, -5.8150e-02,  3.4497e-01,  2.4362e-01,\n",
       "         6.5956e-02,  4.1584e-01,  5.5690e-02, -3.8205e-01, -3.3583e-01,\n",
       "         1.6318e-01, -2.2225e-01, -5.1942e-01, -2.0935e-01, -5.9169e-04,\n",
       "        -2.2722e-01, -8.3223e-02, -9.0506e-01,  4.7829e-02, -2.2481e-01,\n",
       "        -5.1506e-01, -2.5404e-01, -2.2716e-01,  6.8129e-02, -1.2510e-01,\n",
       "         5.0658e-01, -7.5933e-02,  2.9572e-01,  1.8358e-01,  8.1374e-02,\n",
       "         1.2371e-01,  3.6819e-01, -1.4844e-01,  2.3992e-01,  1.9439e-01,\n",
       "        -4.2310e-03, -9.8127e-02, -4.1655e-01,  1.4536e-01, -2.1078e-01,\n",
       "        -7.1932e-02,  2.5833e-01, -7.6533e-02,  5.8363e-02, -3.1892e-01,\n",
       "         1.1808e+00,  2.2278e-01, -4.9068e-01, -1.3950e-02,  6.1660e-02,\n",
       "        -3.4789e-01,  2.6349e-01, -1.8886e-02,  6.6487e-03, -2.1081e-01,\n",
       "        -9.8147e-02, -4.8408e-01, -1.6654e-01, -1.3071e-01, -1.9858e-01,\n",
       "         2.5810e-01,  9.8254e-02,  3.9679e-01,  1.1378e-01,  1.4311e-01,\n",
       "         3.3205e-01,  6.5573e-02, -2.1615e-01, -3.6768e-01, -6.8207e-02,\n",
       "         2.2066e-01,  3.3434e-02, -5.7005e-01, -5.2120e-01,  8.0032e-01,\n",
       "        -3.4849e-01, -9.8540e-02, -3.3687e-01, -1.0740e-01,  5.2592e-02,\n",
       "         1.7180e-01,  2.0665e-01, -2.8985e-01,  2.1083e-01, -1.2008e-01,\n",
       "        -2.9282e-01, -2.4470e-02,  2.0231e-01,  3.1131e-01,  2.2461e-01,\n",
       "         4.3707e-01,  3.0987e-01, -1.4547e-01,  5.6459e-01,  3.4809e-01,\n",
       "        -5.2619e-01, -4.3558e-01, -1.0438e-01,  6.4649e-01, -1.4317e-01,\n",
       "         1.7085e-01,  2.4945e-01, -2.9975e-01,  1.8607e-01, -6.9873e-03,\n",
       "        -5.6955e-01,  9.9169e-02,  8.5767e-01, -2.6643e-01, -4.1561e-01,\n",
       "        -2.2321e-01, -8.4673e-03, -3.7619e-01, -1.3529e-01, -2.5705e-01,\n",
       "        -4.6550e-02,  2.4618e-01,  4.3575e-01,  6.9223e-01, -6.2545e-02,\n",
       "         7.4626e-02,  2.0036e-01, -3.3616e-01, -3.9831e-01,  1.5154e-01,\n",
       "         2.8466e-02, -2.5144e-01, -2.1760e-01, -8.9473e+00,  3.3106e-01,\n",
       "        -2.5859e-02,  1.4435e-01,  1.1831e-01, -1.1906e-01,  5.5894e-02,\n",
       "        -4.1875e-01, -5.5400e-01,  1.8952e-01, -5.6719e-01, -3.6589e-01,\n",
       "         3.0183e-01,  3.1845e-03,  3.1783e-01, -9.0441e-01,  5.6463e-01,\n",
       "        -4.0119e-01,  1.2228e-01,  1.6833e-01, -1.9178e-01, -1.9847e-01,\n",
       "        -1.3589e-01,  1.8843e-01,  5.9871e-01,  4.4969e-01, -3.4238e-01,\n",
       "         7.2676e-02,  2.1505e-01,  1.3726e-01, -3.6002e-01,  1.4663e-01,\n",
       "         8.4500e-02,  5.6917e-01,  1.3637e-01,  2.7043e-01, -5.4669e-01,\n",
       "        -3.8554e-01,  7.5591e-02,  3.2554e-01, -4.3924e-02, -3.5126e-01,\n",
       "        -2.3049e-01,  1.9951e-01,  1.1325e+00, -8.2941e-02,  2.3095e-01,\n",
       "        -4.0543e-01,  2.0420e-01,  2.8932e-01,  4.4851e-02, -3.7761e-01,\n",
       "        -1.9881e-01,  3.9190e-02,  2.4908e-01, -6.0441e-02, -1.9218e-02,\n",
       "         7.5560e-01,  1.5298e-02, -4.9986e-01, -3.6531e-02, -6.1547e-01,\n",
       "        -2.2399e-01, -1.8844e-02, -3.1715e-01,  1.7011e-01, -2.7965e-01,\n",
       "        -8.1915e-02, -2.2264e-02, -6.1725e-01, -1.9776e-01,  2.9320e-01,\n",
       "        -2.3920e-01, -1.4421e+00, -3.4302e-01,  2.5604e-01,  6.2794e-01,\n",
       "        -1.2472e-01, -3.9030e-01, -2.7106e-01, -2.4761e-01, -2.3689e-01,\n",
       "         1.6951e-01, -2.4831e-01, -1.0589e-01, -2.0470e-01, -2.3709e-01,\n",
       "         4.0435e-01, -7.8333e-01, -2.2029e-01,  4.1360e-01,  4.7224e-01,\n",
       "        -2.1184e-02, -1.5510e-02,  1.0123e-02,  2.6490e-01, -3.8680e-02,\n",
       "        -3.9653e-01, -5.4572e-02,  2.5597e-01,  5.2606e-02,  3.4635e-02,\n",
       "         1.8286e-01,  8.5918e-02, -4.5761e-01, -5.4992e-01, -6.8103e-02,\n",
       "        -2.0199e-02, -8.0112e-01,  1.6331e-01, -6.0741e-02, -3.8147e-01,\n",
       "         5.8291e-01,  4.4306e-01, -8.9014e-02,  2.3069e-01,  4.9252e-01,\n",
       "        -2.2887e-02,  2.3276e-02, -8.2784e-02, -1.7635e-01,  3.7576e-01,\n",
       "        -4.2754e-01, -8.5595e-01,  6.7222e-02, -7.0307e-02, -4.7802e-02,\n",
       "        -2.7673e-01, -2.5070e-01,  2.1425e-01, -1.5991e-02, -8.1192e-02,\n",
       "        -3.2587e-01,  3.9200e-02,  3.5547e-01, -3.4536e-02,  2.7662e-01,\n",
       "        -1.1505e+00,  1.8823e-01,  2.9987e-01, -6.1769e-02,  1.1317e-01,\n",
       "         4.8480e-01,  2.9384e-01, -3.5531e-01, -3.1733e-02,  3.3033e-01,\n",
       "        -2.3014e-02, -6.5890e-02,  2.6381e-01, -6.8673e-01, -4.6139e-01,\n",
       "         3.2715e-01,  3.0005e-02,  1.8041e-01,  3.2988e-01,  5.8914e-01,\n",
       "        -2.8886e-01, -3.9456e-01, -5.2439e-02,  4.8494e-01,  7.7459e-02,\n",
       "         4.6351e-01,  5.0227e-01, -8.5998e-01,  3.5623e-01, -8.9036e-01,\n",
       "        -6.7843e-02,  3.2450e-01,  4.8584e-01, -2.5600e-02, -1.8386e-01,\n",
       "         9.4516e-02,  4.7309e-01,  2.8381e-01,  3.5980e-01,  4.0563e-01,\n",
       "        -1.7811e-01, -3.2982e-01,  1.7885e-02, -1.2815e-01,  4.6590e-01,\n",
       "        -3.0801e-01, -5.2276e-01,  4.7493e-01,  1.0808e-01,  1.4295e-01,\n",
       "         2.2118e-01, -1.6154e-01, -8.8219e-02, -3.7619e-01,  5.1643e-01,\n",
       "        -9.0325e-02,  1.5615e-01, -3.9942e-01, -6.6670e-01, -7.5806e-02,\n",
       "        -6.0950e-02, -2.4386e-01,  4.6069e-01,  5.5020e-02, -2.3349e-01,\n",
       "         7.3342e-02, -1.5700e-01, -5.3928e-01, -2.7322e-01,  3.2698e-01,\n",
       "         2.3594e-01, -2.2926e-01,  5.2219e-01, -4.4231e-01, -1.6537e-01,\n",
       "        -3.1332e-01, -7.1890e-01,  2.5224e-02, -1.5102e-01,  4.2354e-01,\n",
       "        -1.4992e-01, -5.8868e-01,  4.9081e-03, -7.8717e-02,  7.8058e-02,\n",
       "        -2.1630e-01,  1.2895e-01, -6.0102e-01,  3.2167e-02,  2.0908e-01,\n",
       "         7.0302e-02, -2.4196e-02,  1.3994e-01,  7.3034e-02, -1.2279e+00,\n",
       "         3.3817e-02, -1.2572e-02, -4.2208e-02, -1.3200e-01, -8.3746e-02,\n",
       "        -6.9221e-01, -1.7280e-01, -2.8515e-01, -1.1447e-01, -1.6212e-01,\n",
       "         1.0644e-01,  8.8729e-02,  2.3815e-01,  3.8417e-01,  1.1063e-01,\n",
       "         1.1019e-01, -1.5872e-01, -2.0006e-01,  6.4351e-01, -1.0937e-01,\n",
       "         9.0504e-02,  8.6393e-03, -4.0593e-01, -1.5051e-01, -4.1752e-01,\n",
       "        -3.0533e-02,  1.6019e-01,  3.1481e-01,  2.2252e-01, -7.0266e-01,\n",
       "        -1.4060e-02,  1.8591e-01,  3.5740e-01, -3.0161e-01,  2.4112e-01,\n",
       "         6.2852e-01, -4.2949e-01, -2.4458e-01,  2.2510e-02, -6.2274e-01,\n",
       "         4.0334e-01,  8.1106e-02, -5.4198e-02,  2.1060e-01, -4.9498e-02,\n",
       "        -9.5785e-01, -4.6977e-02, -4.6892e-01, -2.7845e-01,  2.7449e-01,\n",
       "         4.1954e-01,  5.3567e-02, -2.0983e-01, -2.0536e-01, -3.0980e-01,\n",
       "         5.5660e-01, -2.9674e-01, -4.1906e-01,  2.3740e-01,  3.0965e-01,\n",
       "         1.7259e-01, -1.9691e-01,  8.9610e-02,  4.9515e-01,  8.2106e-01,\n",
       "         2.3962e-01,  9.5271e-02,  5.1891e-01, -2.5714e-01, -2.2466e-01,\n",
       "         1.4482e-01,  2.2612e-01, -1.2185e-01,  2.4755e-01, -2.9429e-03,\n",
       "        -1.5971e-01,  3.4729e-01, -4.4140e-01, -4.2048e-01, -3.4176e-01,\n",
       "         9.6706e-02,  2.2398e-01, -4.7847e-01, -7.0723e-01, -3.0114e-01,\n",
       "        -2.5278e-01, -6.5723e-01,  2.2664e-01,  1.5600e-01, -8.2870e-02,\n",
       "         4.0645e-01, -2.8966e-02,  9.9876e-02,  4.7404e-01, -6.0977e-02,\n",
       "        -2.1549e-01, -1.3406e-02,  2.0672e-01, -3.7132e-01,  5.8330e-03,\n",
       "         4.0122e-02, -1.3517e-01, -5.2835e-02,  2.4097e-01,  3.3337e-02,\n",
       "        -7.0154e-01,  4.2875e-02, -6.8544e-01,  1.0008e-01, -1.0984e-01,\n",
       "        -2.4321e-01,  5.8598e-01,  3.7942e-01, -2.9073e-01, -2.6446e-01,\n",
       "        -3.2885e-01,  4.3894e-01,  1.3584e-01, -1.6084e-01,  4.5264e-02,\n",
       "         1.2735e-01, -2.4946e-01,  9.2901e-01,  1.2621e-01, -6.2723e-02,\n",
       "        -3.0043e-02, -3.2415e-01,  3.7303e-01,  4.6272e-01, -2.2281e-02,\n",
       "         2.1766e-01, -5.6937e-01,  1.8324e-01,  8.0259e-02, -1.6683e-01,\n",
       "        -5.1343e-01, -3.0841e-02,  4.8097e-02,  2.5198e-01, -8.0760e-02,\n",
       "         4.3488e-01, -5.8684e-01,  2.2417e-01,  1.0856e-01,  3.5096e-02,\n",
       "         7.6372e-02,  1.8075e-01, -1.4464e-01, -1.3676e-01,  1.5800e-01,\n",
       "        -7.5601e-01, -7.3623e-01, -1.9872e-01, -5.7531e-02,  5.4665e-03,\n",
       "        -1.5238e-01, -1.5560e-01, -1.7286e-01,  8.6270e-02, -4.2647e-02,\n",
       "         1.1482e-01, -1.5104e-01,  1.5394e-01, -1.5096e-01,  1.0028e-01,\n",
       "        -1.0733e-01,  3.2950e-01,  3.7414e-02,  5.1107e-02, -2.0950e-01,\n",
       "         1.6382e-01, -1.6224e-02, -6.1106e-01, -1.7929e-01, -1.1188e-01,\n",
       "         1.2675e-01, -6.5118e-01,  3.8737e-02,  1.1624e-01,  3.2449e-01,\n",
       "         4.9853e-02,  3.3065e-01,  4.1722e-01, -3.2470e-01,  1.7247e-01,\n",
       "        -2.0100e-01, -4.5544e-01, -1.4141e-01, -4.3142e-01, -2.9520e-01,\n",
       "        -6.2154e-03,  1.0913e-01, -8.7277e-02, -8.0374e-01, -1.5279e-01,\n",
       "        -1.5507e-01,  8.8968e-02,  2.4266e-01, -6.2637e-02, -6.4045e-02,\n",
       "         4.0773e-01,  2.9100e-01,  1.8645e-02, -2.0947e-01, -2.9651e-01,\n",
       "         6.3428e-01,  1.2118e-01,  4.3935e-01,  2.0832e-01,  9.3167e-02,\n",
       "         2.9057e-01,  5.6046e-02,  1.5043e-01,  1.6326e-01, -2.3932e-01,\n",
       "        -1.9242e-02, -4.8570e-02, -1.8352e-01, -3.4966e-01, -1.5879e-02,\n",
       "         1.0726e-01, -8.3273e-02, -2.5004e-01,  2.5290e-01, -3.6050e-01,\n",
       "        -4.1351e-01, -2.2482e-01, -1.9857e-01])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "print(\"Our final sentence embedding vector of shape: \", sentence_embedding.size())\n",
    "\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
